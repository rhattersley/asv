<!DOCTYPE html>


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Using airspeed velocity &mdash; airspeed velocity 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootstrap-3.2.0/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootstrap-3.2.0/css/bootstrap-theme.min.css" type="text/css" />
    <link rel="stylesheet" href="_static/bootstrap-sphinx.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/js/jquery-1.11.0.min.js"></script>
    <script type="text/javascript" src="_static/js/jquery-fix.js"></script>
    <script type="text/javascript" src="_static/bootstrap-3.2.0/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="_static/bootstrap-sphinx.js"></script>
    <link rel="shortcut icon" href="_static/swallow.ico"/>
    <link rel="top" title="airspeed velocity 0.1 documentation" href="index.html" />
    <link rel="next" title="Writing benchmarks" href="writing_benchmarks.html" />
    <link rel="prev" title="Installing airspeed velocity" href="installing.html" />
<meta charset='utf-8'>
<meta http-equiv='X-UA-Compatible' content='IE=edge,chrome=1'>
<meta name='viewport' content='width=device-width, initial-scale=1.0, maximum-scale=1'>
<meta name="apple-mobile-web-app-capable" content="yes">

  </head>
  <body>
  
  <a href="https://github.com/spacetelescope/asv"
     class="visible-desktop"><img
    style="position: absolute; top: 40px; right: 0; border: 0;"
    src="https://s3.amazonaws.com/github/ribbons/forkme_right_white_ffffff.png"
    alt="Fork me on GitHub"></a>


  <div id="navbar" class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <!-- .btn-navbar is used as the toggle for collapsed navbar content -->
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".nav-collapse">
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="index.html">
          airspeed velocity</a>
        <span class="navbar-text navbar-version pull-left"><b>0.1</b></span>
      </div>

        <div class="collapse navbar-collapse nav-collapse">
          <ul class="nav navbar-nav">
            
            
              <li class="dropdown globaltoc-container">
  <a role="button"
     id="dLabelGlobalToc"
     data-toggle="dropdown"
     data-target="#"
     href="index.html">Site <b class="caret"></b></a>
  <ul class="dropdown-menu globaltoc"
      role="menu"
      aria-labelledby="dLabelGlobalToc"><ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installing.html">Installing airspeed velocity</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="">Using airspeed velocity</a></li>
<li class="toctree-l1"><a class="reference internal" href="writing_benchmarks.html">Writing benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="reference.html">Reference</a></li>
</ul>
</ul>
</li>
              
                <li class="dropdown">
  <a role="button"
     id="dLabelLocalToc"
     data-toggle="dropdown"
     data-target="#"
     href="#">Page <b class="caret"></b></a>
  <ul class="dropdown-menu localtoc"
      role="menu"
      aria-labelledby="dLabelLocalToc"><ul>
<li><a class="reference internal" href="#">Using airspeed velocity</a><ul>
<li><a class="reference internal" href="#setting-up-a-new-benchmarking-project">Setting up a new benchmarking project</a></li>
<li><a class="reference internal" href="#running-benchmarks">Running benchmarks</a><ul>
<li><a class="reference internal" href="#machine-information">Machine information</a></li>
<li><a class="reference internal" href="#environments">Environments</a></li>
<li><a class="reference internal" href="#benchmarking">Benchmarking</a></li>
</ul>
</li>
<li><a class="reference internal" href="#viewing-the-results">Viewing the results</a></li>
<li><a class="reference internal" href="#managing-the-results-database">Managing the results database</a></li>
<li><a class="reference internal" href="#finding-a-commit-that-produces-a-large-regression">Finding a commit that produces a large regression</a></li>
<li><a class="reference internal" href="#running-a-benchmark-in-the-profiler">Running a benchmark in the profiler</a></li>
<li><a class="reference internal" href="#comparing-the-benchmarking-results-for-two-revisions">Comparing the benchmarking results for two revisions</a></li>
</ul>
</li>
</ul>
</ul>
</li>
              
            
            
              
                
  <li>
    <a href="installing.html" title="Previous Chapter: Installing airspeed velocity"><span class="glyphicon glyphicon-chevron-left visible-sm"></span><span class="hidden-sm">&laquo; Installing airsp...</span>
    </a>
  </li>
  <li>
    <a href="writing_benchmarks.html" title="Next Chapter: Writing benchmarks"><span class="glyphicon glyphicon-chevron-right visible-sm"></span><span class="hidden-sm">Writing benchmar... &raquo;</span>
    </a>
  </li>
              
            
            
            
            
              <li class="hidden-sm"></li>
            
          </ul>

          
            
<form class="navbar-form navbar-right" action="search.html" method="get">
 <div class="form-group">
  <input type="text" name="q" class="form-control" placeholder="Search" />
 </div>
  <input type="hidden" name="check_keywords" value="yes" />
  <input type="hidden" name="area" value="default" />
</form>
          
        </div>
    </div>
  </div>

<div class="container">
  <div class="row">
    <div class="col-md-12">
      
  <div class="section" id="using-airspeed-velocity">
<h1>Using airspeed velocity<a class="headerlink" href="#using-airspeed-velocity" title="Permalink to this headline">¶</a></h1>
<p><strong>airspeed velocity</strong> is designed to benchmark a single project over
its lifetime using a given set of benchmarks.  Below, we use the
phrase &#8220;project&#8221; to refer to the project being benchmarked, and
&#8220;benchmark suite&#8221; to refer to the set of benchmarks &#8211; i.e., little
snippets of code that are timed &#8211; being run against the project.  The
benchmark suite may live inside the project&#8217;s repository, or it may
reside in a separate repository &#8211; the choice is up to you and is
primarily a matter of style or policy.  Importantly, the result data
stored alongside the benchmark suite may grow quite large, which is a
good reason to not include it in the main project repository.</p>
<p>The user interacts with <strong>airspeed velocity</strong> through the <tt class="docutils literal"><span class="pre">asv</span></tt>
command.  Like <tt class="docutils literal"><span class="pre">git</span></tt>, the <tt class="docutils literal"><span class="pre">asv</span></tt> command has a number of
&#8220;subcommands&#8221; for performing various actions on your benchmarking
project.</p>
<div class="section" id="setting-up-a-new-benchmarking-project">
<h2>Setting up a new benchmarking project<a class="headerlink" href="#setting-up-a-new-benchmarking-project" title="Permalink to this headline">¶</a></h2>
<p>The first thing to do is to set up an <strong>airspeed velocity</strong> benchmark
suite for your project.  It must contain, at a minimum, a single
configuration file, <tt class="docutils literal"><span class="pre">asv.conf.json</span></tt>, and a directory tree of Python
files containing benchmarks.</p>
<p>The <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">quickstart</span></tt> command can be used to create a new
benchmarking suite.  Change to the directory where you would like your
new benchmarking suite to be created and run:</p>
<div class="highlight-python"><div class="highlight"><pre>$ asv quickstart
Edit asv.conf.json to get started.
</pre></div>
</div>
<p>Now that you have the bare bones of a benchmarking suite, let&#8217;s edit
the configuration file, <tt class="docutils literal"><span class="pre">asv.conf.json</span></tt>.  Like most files that
<strong>airspeed velocity</strong> uses and generates, it is a JSON file.</p>
<p>There are comments in the file describing what each of the elements
do, and there is also a <a class="reference internal" href="asv.conf.json.html#conf-reference"><em>asv.conf.json reference</em></a> with more details.  The
values that will most likely need to be changed for any benchmarking
suite are:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">project</span></tt>: The name of the project being benchmarked.</li>
<li><tt class="docutils literal"><span class="pre">project_url</span></tt>: The project&#8217;s homepage.</li>
<li><tt class="docutils literal"><span class="pre">repo</span></tt>: The URL to the DVCS repository for the project.  This
should be a read-only URL so that anyone, even those without commit
rights to the repository, can run the benchmarks.  For a project on
github, for example, the URL would look like:
<tt class="docutils literal"><span class="pre">https://github.com/spacetelescope/asv.git</span></tt></li>
<li><tt class="docutils literal"><span class="pre">show_commit_url</span></tt>: The base of URLs used to display commits for
the project.  This allows users to click on a commit in the web
interface and have it display the contents of that commit.  For a
github project, the URL is of the form
<tt class="docutils literal"><span class="pre">http://github.com/$OWNER/$REPO/commit/</span></tt>.</li>
</ul>
<p>The rest of the values can usually be left to their defaults, unless
you want to benchmark against multiple versions of Python or multiple
versions of third-party dependencies.</p>
<p>Once you&#8217;ve set up the project&#8217;s configuration, you&#8217;ll need to write
some benchmarks.  The benchmarks live in Python files in the
<tt class="docutils literal"><span class="pre">benchmarks</span></tt> directory.  The <tt class="docutils literal"><span class="pre">quickstart</span></tt> command has created a
single example benchmark file already in
<tt class="docutils literal"><span class="pre">benchmarks/benchmarks.py</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">class</span> <span class="nc">TimeSuite</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An example benchmark that times the performance of various kinds</span>
<span class="sd">    of iterating over dictionaries in Python.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="p">[</span><span class="n">x</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">time_keys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">time_iterkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span><span class="o">.</span><span class="n">iterkeys</span><span class="p">():</span>
            <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">time_range</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">d</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">time_xrange</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">d</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">d</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">d</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
</pre></div>
</div>
<p>You&#8217;ll want to replace these benchmarks with your own.  See
<a class="reference internal" href="writing_benchmarks.html#writing-benchmarks"><em>Writing benchmarks</em></a> for more information.</p>
</div>
<div class="section" id="running-benchmarks">
<h2>Running benchmarks<a class="headerlink" href="#running-benchmarks" title="Permalink to this headline">¶</a></h2>
<p>Benchmarks are run using the <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">run</span></tt> subcommand.</p>
<p>Let&#8217;s start by just benchmarking the latest commit on the current
<tt class="docutils literal"><span class="pre">master</span></tt> branch of the project:</p>
<div class="highlight-python"><div class="highlight"><pre>$ asv run
</pre></div>
</div>
<div class="section" id="machine-information">
<h3>Machine information<a class="headerlink" href="#machine-information" title="Permalink to this headline">¶</a></h3>
<p>If this is the first time using <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">run</span></tt> on a given machine, (which
it probably is, if you&#8217;re following along), you will be prompted for
information about the machine, such as its platform, cpu and memory.
<strong>airspeed velocity</strong> will try to make reasonable guesses, so it&#8217;s
usually ok to just press <tt class="docutils literal"><span class="pre">Enter</span></tt> to accept each default value.  This
information is stored in the <tt class="docutils literal"><span class="pre">~/.asv-machine.json</span></tt> file in your home
directory:</p>
<div class="highlight-python"><div class="highlight"><pre>I will now ask you some questions about this machine to identify
it in the benchmarks.

1. machine: A unique name to identify this machine in the results.
   May be anything, as long as it is unique across all the
   machines used to benchmark this project.  NOTE: If changed from
   the default, it will no longer match the hostname of this
   machine, and you may need to explicitly use the --machine
   argument to asv.
machine [cheetah]:
2. os: The OS type and version of this machine.  For example,
   &#39;Macintosh OS-X 10.8&#39;.
os [Linux 3.17.6-300.fc21.x86_64]:
3. arch: The generic CPU architecture of this machine.  For
   example, &#39;i386&#39; or &#39;x86_64&#39;.
arch [x86_64]:
4. cpu: A specific description of the CPU of this machine,
   including its speed and class.  For example, &#39;Intel(R) Core(TM)
   i5-2520M CPU @ 2.50GHz (4 cores)&#39;.
cpu [Intel(R) Core(TM) i5-2520M CPU @ 2.50GHz]:
5. ram: The amount of physical RAM on this machine.  For example,
   &#39;4GB&#39;.
ram [8055476]:
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you ever need to update the machine information later, you can
run <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">machine</span></tt>.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">By default, the name of the machine is determined from your
hostname.  If you have a hostname that frequently changes, and
your <tt class="docutils literal"><span class="pre">~/.asv-machine.json</span></tt> file contains more than one entry,
you will need to use the <tt class="docutils literal"><span class="pre">--machine</span></tt> argument to <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">run</span></tt> and
similar commands.</p>
</div>
</div>
<div class="section" id="environments">
<h3>Environments<a class="headerlink" href="#environments" title="Permalink to this headline">¶</a></h3>
<p>Next, the Python environments to run the benchmarks are set up.  One
environment will be set up for each of the combinations of Python
versions and the matrix of project dependencies, if any.  The first
time this is run, this may take some time, as many files are copied
over and dependencies are installed into the environment.  The
environments are stored in the <tt class="docutils literal"><span class="pre">env</span></tt> directory so that the next time
the benchmarks are run, things will start much faster.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The easiest way to benchmark in multiple versions of Python is to
install <a class="reference external" href="https://store.continuum.io/cshop/anaconda/">anaconda</a>
or <a class="reference external" href="http://conda.pydata.org/miniconda.html">miniconda</a>
installation, and make sure the <tt class="docutils literal"><span class="pre">conda</span></tt> command is available on
your <tt class="docutils literal"><span class="pre">PATH</span></tt>.</p>
<p>If not using Conda, virtualenv will be used.  In this case,
<tt class="docutils literal"><span class="pre">asv</span></tt> does not build Python interpreters for you, but it expects
to find each of the Python versions specified in the
<tt class="docutils literal"><span class="pre">asv.conf.json</span></tt> file available on the <tt class="docutils literal"><span class="pre">PATH</span></tt>.  For example, if
the <tt class="docutils literal"><span class="pre">asv.conf.json</span></tt> file has:</p>
<div class="highlight-python"><div class="highlight"><pre>&quot;pythons&quot;: [&quot;2.7&quot;, &quot;3.3&quot;]
</pre></div>
</div>
<p class="last">then it will use the executables named <tt class="docutils literal"><span class="pre">python2.7</span></tt> and
<tt class="docutils literal"><span class="pre">python3.3</span></tt> on the path.  There are many ways to get multiple
versions of Python installed &#8211; your package manager, <tt class="docutils literal"><span class="pre">apt-get</span></tt>,
<tt class="docutils literal"><span class="pre">yum</span></tt>, <tt class="docutils literal"><span class="pre">MacPorts</span></tt> or <tt class="docutils literal"><span class="pre">homebrew</span></tt> probably has them, or you
can also use <a class="reference external" href="https://github.com/yyuu/pyenv">pyenv</a>.  <tt class="docutils literal"><span class="pre">asv</span></tt>
always runs its benchmarks in a virtual environment, so it will
not change what is installed in any of the python environments on
your system.</p>
</div>
</div>
<div class="section" id="benchmarking">
<h3>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this headline">¶</a></h3>
<p>Finally, the benchmarks are run:</p>
<div class="highlight-python"><div class="highlight"><pre>$ asv run
· Cloning project.
· Fetching recent changes..
· Creating environments
·· Creating conda environment for py2.7
·· Creating conda environment for py3.4
· Installing dependencies..
· Discovering benchmarks
·· Creating conda environment for py2.7
·· Uninstalling project from py2.7
·· Installing project into py2.7.
· Running 10 total benchmarks (1 commits * 2 environments * 5 benchmarks)
[  0.00%] · For project commit hash ac71c70d:
[  0.00%] ·· Building for py2.7
[  0.00%] ··· Uninstalling project from py2.7
[  0.00%] ··· Installing project into py2.7.
[  0.00%] ·· Benchmarking py2.7
[ 10.00%] ··· Running benchmarks.MemSuite.mem_list                               2.4k
[ 20.00%] ··· Running benchmarks.TimeSuite.time_iterkeys                       9.27μs
[ 30.00%] ··· Running benchmarks.TimeSuite.time_keys                          10.74μs
[ 40.00%] ··· Running benchmarks.TimeSuite.time_range                         42.20μs
[ 50.00%] ··· Running benchmarks.TimeSuite.time_xrange                        32.94μs
[ 50.00%] ·· Building for py3.4
[ 50.00%] ··· Uninstalling project from py3.4
[ 50.00%] ··· Installing project into py3.4..
[ 50.00%] ·· Benchmarking py3.4
[ 60.00%] ··· Running benchmarks.MemSuite.mem_list                               2.4k
[ 70.00%] ··· Running benchmarks.TimeSuite.time_iterkeys                     failed
[ 80.00%] ··· Running benchmarks.TimeSuite.time_keys                           7.29μs
[ 90.00%] ··· Running benchmarks.TimeSuite.time_range                         30.41μs
[100.00%] ··· Running benchmarks.TimeSuite.time_xrange                       failed
</pre></div>
</div>
<p>To improve reproducibility, each benchmark is run in its own process.</p>
<p>The killer feature of <strong>airspeed velocity</strong> is that it can track the
benchmark performance of your project over time.  The <tt class="docutils literal"><span class="pre">range</span></tt>
argument to <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">run</span></tt> specifies a range of commits that should be
benchmarked.  The value of this argument is passed directly to <tt class="docutils literal"><span class="pre">git</span>
<span class="pre">log</span></tt> to get the set of commits, so it actually has a very powerful
syntax defined in the <a class="reference external" href="https://www.kernel.org/pub/software/scm/git/docs/gitrevisions.html">gitrevisions manpage</a>.</p>
<p>For example, one can test a range of commits on a particular branch
since the branch was created:</p>
<div class="highlight-python"><div class="highlight"><pre>asv run mybranch@{u}..mybranch
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Yes, this is git-specific for now.  Support for Mercurial or other
DVCSes should be possible in the future, but not at the moment.</p>
</div>
<p>For example, to benchmark all of the commits since a particular tag
(<tt class="docutils literal"><span class="pre">v0.1</span></tt>):</p>
<div class="highlight-python"><div class="highlight"><pre>asv run v0.1..master
</pre></div>
</div>
<p>In many cases, this may result in more commits than you are able to
benchmark in a reasonable amount of time.  In that case, the
<tt class="docutils literal"><span class="pre">--steps</span></tt> argument is helpful.  It specifies the maximum number of
commits you want to test, and it will evenly space them over the
specified range.</p>
<p>You can benchmark all commits in the repository by using:</p>
<div class="highlight-python"><div class="highlight"><pre>asv run ALL
</pre></div>
</div>
<p>You may also want to benchmark every commit that has already been
benchmarked on all the other machines.  For that, use:</p>
<div class="highlight-python"><div class="highlight"><pre>asv run EXISTING
</pre></div>
</div>
<p>You can benchmark all commits since the last one that was benchmarked
on this machine.  This is useful for running in nightly cron jobs:</p>
<div class="highlight-python"><div class="highlight"><pre>asv run NEW
</pre></div>
</div>
<p>Finally, you can also benchmark all commits that have not yet been benchmarked
for this machine:</p>
<div class="highlight-python"><div class="highlight"><pre>asv run MISSING
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">There is a special version of <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">run</span></tt> that is useful when
developing benchmarks, called <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">dev</span></tt>.  See
<a class="reference internal" href="writing_benchmarks.html#writing-benchmarks"><em>Writing benchmarks</em></a> for more information.</p>
</div>
<p>The results are stored as a tree of files in the directory
<tt class="docutils literal"><span class="pre">results/$MACHINE</span></tt>, where <tt class="docutils literal"><span class="pre">$MACHINE</span></tt> is the unique machine name
that was set up in your <tt class="docutils literal"><span class="pre">~/.asv-machine.json</span></tt> file.  In order to
combine results from multiple machines, the normal workflow is to
commit these results to a source code repository alongside the results
from other machines.  These results are then collated and &#8220;published&#8221;
altogether into a single interactive website for viewing (see
<a class="reference internal" href="#viewing-results"><em>Viewing the results</em></a>).</p>
<p>You can also continue to generate benchmark results for other commits,
or for new benchmarks and continue to throw them in the <tt class="docutils literal"><span class="pre">results</span></tt>
directory.  <strong>airspeed velocity</strong> is designed from the ground up to
handle missing data where certain benchmarks have yet to be performed
&#8211; it&#8217;s entirely up to you how often you want to generate results, and
on which commits and in which configurations.</p>
</div>
</div>
<div class="section" id="viewing-the-results">
<span id="viewing-results"></span><h2>Viewing the results<a class="headerlink" href="#viewing-the-results" title="Permalink to this headline">¶</a></h2>
<p>To collate a set of results into a viewable website, run:</p>
<div class="highlight-python"><div class="highlight"><pre>asv publish
</pre></div>
</div>
<p>This will put a tree of files in the <tt class="docutils literal"><span class="pre">html</span></tt> directory.  This website
can not be viewed directly from the local filesystem, since web
browsers do not support AJAX requests to the local filesystem.
Instead, <strong>airspeed velocity</strong> provides a simple static webserver that
can be used to preview the website.  Just run:</p>
<div class="highlight-python"><div class="highlight"><pre>asv preview
</pre></div>
</div>
<p>and open the URL that is displayed at the console.  Press Ctrl+C to
stop serving.</p>
<img alt="_images/screenshot.png" src="_images/screenshot.png" />
<p>To share the website on the open internet, simply put these files on
any webserver that can serve static content.  Github Pages works quite
well, for example.  If using Github Pages, asv includes the
convenience command <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">gh-pages</span></tt> to automatically publish the
results to the <tt class="docutils literal"><span class="pre">gh-pages</span></tt> branch.</p>
</div>
<div class="section" id="managing-the-results-database">
<h2>Managing the results database<a class="headerlink" href="#managing-the-results-database" title="Permalink to this headline">¶</a></h2>
<p>The <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">rm</span></tt> command can be used to remove benchmarks from the
database.  The command takes an arbitrary number of <tt class="docutils literal"><span class="pre">key=value</span></tt>
entries that are &#8220;and&#8221;ed together to determine which benchmarks to
remove.</p>
<p>The keys may be one of:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">benchmark</span></tt>: A benchmark name</li>
<li><tt class="docutils literal"><span class="pre">python</span></tt>: The version of python</li>
<li><tt class="docutils literal"><span class="pre">commit_hash</span></tt>: The commit hash</li>
<li>machine-related: <tt class="docutils literal"><span class="pre">machine</span></tt>, <tt class="docutils literal"><span class="pre">arch</span></tt>, <tt class="docutils literal"><span class="pre">cpu</span></tt>, <tt class="docutils literal"><span class="pre">os</span></tt>, <tt class="docutils literal"><span class="pre">ram</span></tt></li>
<li>environment-related: a name of a dependency, e.g. <tt class="docutils literal"><span class="pre">numpy</span></tt></li>
</ul>
<p>The values are glob patterns, as supported by the Python standard
library module <a class="reference external" href="http://docs.python.org/library/fnmatch.html#module-fnmatch" title="(in Python v2.7)"><tt class="xref py py-obj docutils literal"><span class="pre">fnmatch</span></tt></a>.  So, for example, to remove all benchmarks
in the <tt class="docutils literal"><span class="pre">time_units</span></tt> module:</p>
<div class="highlight-python"><div class="highlight"><pre>asv rm &quot;benchmark=time_units.*&quot;
</pre></div>
</div>
<p>Note the double quotes around the entry to prevent the shell from
expanding the <tt class="docutils literal"><span class="pre">*</span></tt> itself.</p>
<p>The <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">rm</span></tt> command will prompt before performing any operations.
Passing the <tt class="docutils literal"><span class="pre">-y</span></tt> option will skip the prompt.  Note that generally
the results will be stored in a source code repository, so it should
be possible to undo any of the changes using the DVCS directly as
well.</p>
<p>Here is a more complex example, to remove all of the benchmarks on
Python 2.7 and the machine named <tt class="docutils literal"><span class="pre">giraffe</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre>asv rm python=2.7 machine=giraffe
</pre></div>
</div>
</div>
<div class="section" id="finding-a-commit-that-produces-a-large-regression">
<h2>Finding a commit that produces a large regression<a class="headerlink" href="#finding-a-commit-that-produces-a-large-regression" title="Permalink to this headline">¶</a></h2>
<p>Since benchmarking can be rather time consuming, it&#8217;s likely that
you&#8217;re only benchmarking a subset of all commits in the repository.
When you discover from the graph that the runtime between commit A and
commit B suddenly doubles, you don&#8217;t know which particular commit in
that range is the likely culprit.  <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">find</span></tt> can be used to help
find a commit within that range that produced a large regression using
a binary search.  You can select a range of commits easily from the
web interface by dragging a box around the commits in question.  The
commit hashes associated with that range is then displayed in the
&#8220;commits&#8221; section of the sidebar.  We&#8217;ll copy and paste this commit
range into the commandline arguments of the <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">find</span></tt> command,
along with the name of a single benchmark to use.  The output below is
truncated to show how the search progresses:</p>
<div class="highlight-python"><div class="highlight"><pre>$ asv find 05d4f83d..b96fcc53 time_coordinates.time_latitude
- Running approximately 10 benchmarks within 1156 commits
- Testing &lt;----------------------------O-----------------------------&gt;
- Testing &lt;-------------O--------------&gt;------------------------------
- Testing --------------&lt;-------O------&gt;------------------------------
- Testing --------------&lt;---O---&gt;-------------------------------------
- Testing --------------&lt;-O-&gt;-----------------------------------------
- Testing --------------&lt;O&gt;-------------------------------------------
- Testing --------------&lt;&gt;--------------------------------------------
- Greatest regression found: 2918f61e
</pre></div>
</div>
<p>The result, <tt class="docutils literal"><span class="pre">2918f61e</span></tt> is the commit found with the largest
regression, using the binary search.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The binary search used by <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">find</span></tt> will only be effective when
the runtimes over the range are more-or-less monotonic.  If there
is a lot of variation within that range, it may find only a local
maximum, rather than the global maximum.  For best results, use a
reasonably small commit range.</p>
</div>
</div>
<div class="section" id="running-a-benchmark-in-the-profiler">
<span id="profiling"></span><h2>Running a benchmark in the profiler<a class="headerlink" href="#running-a-benchmark-in-the-profiler" title="Permalink to this headline">¶</a></h2>
<p><strong>airspeed velocity</strong> can oftentimes tell you <em>if</em> something got
slower, but it can&#8217;t really tell you <em>why</em> it got slower.  That&#8217;s
where a profiler comes in.  <strong>airspeed velocity</strong> has features to
easily run a given benchmark in the Python standard library&#8217;s
<a class="reference external" href="http://docs.python.org/library/profile.html#module-cProfile" title="(in Python v2.7)"><tt class="xref py py-obj docutils literal"><span class="pre">cProfile</span></tt></a> profiler, and then open the profiling data in the tool of
your choice.</p>
<p>The <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">profile</span></tt> command profiles a given benchmark on a given
revision of the project.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can also pass the <tt class="docutils literal"><span class="pre">--profile</span></tt> option to <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">run</span></tt>.  In
addition to running the benchmarks as usual, it also runs them
again in the <a class="reference external" href="http://docs.python.org/library/profile.html#module-cProfile" title="(in Python v2.7)"><tt class="xref py py-obj docutils literal"><span class="pre">cProfile</span></tt></a> profiler and save the results.  <tt class="docutils literal"><span class="pre">asv</span>
<span class="pre">preview</span></tt> will use this data, if found, rather than needing to
profile the benchmark each time.  However, it&#8217;s important to note
that profiler data contains absolute paths to the source code, so
they are generally not portable between machines.</p>
</div>
<p><tt class="docutils literal"><span class="pre">asv</span> <span class="pre">profile</span></tt> takes as arguments the name of the benchmark and the
hash, tag or branch of the project to run it in.  Below is a real
world example of testing the <tt class="docutils literal"><span class="pre">astropy</span></tt> project.  By default, a
simple table summary of profiling results is displayed:</p>
<div class="highlight-python"><div class="highlight"><pre>&gt; asv profile time_units.time_very_simple_unit_parse 10fc29cb

     8700042 function calls in 6.844 seconds

 Ordered by: cumulative time

 ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      1    0.000    0.000    6.844    6.844 asv/benchmark.py:171(method_caller)
      1    0.000    0.000    6.844    6.844 asv/benchmark.py:197(run)
      1    0.000    0.000    6.844    6.844 /usr/lib64/python2.7/timeit.py:201(repeat)
      3    0.000    0.000    6.844    2.281 /usr/lib64/python2.7/timeit.py:178(timeit)
      3    0.104    0.035    6.844    2.281 /usr/lib64/python2.7/timeit.py:96(inner)
 300000    0.398    0.000    6.740    0.000 benchmarks/time_units.py:20(time_very_simple_unit_parse)
 300000    1.550    0.000    6.342    0.000 astropy/units/core.py:1673(__call__)
 300000    0.495    0.000    2.416    0.000 astropy/units/format/generic.py:361(parse)
 300000    1.023    0.000    1.841    0.000 astropy/units/format/__init__.py:31(get_format)
 300000    0.168    0.000    1.283    0.000 astropy/units/format/generic.py:374(_do_parse)
 300000    0.986    0.000    1.115    0.000 astropy/units/format/generic.py:345(_parse_unit)
3000002    0.735    0.000    0.735    0.000 {isinstance}
 300000    0.403    0.000    0.403    0.000 {method &#39;decode&#39; of &#39;str&#39; objects}
 300000    0.216    0.000    0.216    0.000 astropy/units/format/generic.py:32(__init__)
 300000    0.152    0.000    0.188    0.000 /usr/lib64/python2.7/inspect.py:59(isclass)
 900000    0.170    0.000    0.170    0.000 {method &#39;lower&#39; of &#39;unicode&#39; objects}
 300000    0.133    0.000    0.133    0.000 {method &#39;count&#39; of &#39;unicode&#39; objects}
 300000    0.078    0.000    0.078    0.000 astropy/units/core.py:272(get_current_unit_registry)
 300000    0.076    0.000    0.076    0.000 {issubclass}
 300000    0.052    0.000    0.052    0.000 astropy/units/core.py:131(registry)
 300000    0.038    0.000    0.038    0.000 {method &#39;strip&#39; of &#39;str&#39; objects}
 300003    0.037    0.000    0.037    0.000 {globals}
 300000    0.033    0.000    0.033    0.000 {len}
      3    0.000    0.000    0.000    0.000 /usr/lib64/python2.7/timeit.py:143(setup)
      1    0.000    0.000    0.000    0.000 /usr/lib64/python2.7/timeit.py:121(__init__)
      6    0.000    0.000    0.000    0.000 {time.time}
      1    0.000    0.000    0.000    0.000 {min}
      1    0.000    0.000    0.000    0.000 {range}
      1    0.000    0.000    0.000    0.000 {hasattr}
      1    0.000    0.000    0.000    0.000 /usr/lib64/python2.7/timeit.py:94(_template_func)
      3    0.000    0.000    0.000    0.000 {gc.enable}
      3    0.000    0.000    0.000    0.000 {method &#39;append&#39; of &#39;list&#39; objects}
      3    0.000    0.000    0.000    0.000 {gc.disable}
      1    0.000    0.000    0.000    0.000 {method &#39;disable&#39; of &#39;_lsprof.Profiler&#39; objects}
      3    0.000    0.000    0.000    0.000 {gc.isenabled}
      1    0.000    0.000    0.000    0.000 &lt;string&gt;:1(&lt;module&gt;)
</pre></div>
</div>
<p>Navigating these sorts of results can be tricky, and generally you
want to open the results in a GUI tool, such as <a class="reference external" href="http://www.vrplumber.com/programming/runsnakerun/">RunSnakeRun</a>.  By passing
the <tt class="docutils literal"><span class="pre">--gui=runsnake</span></tt> to <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">profile</span></tt>, the profile is collected
(or extracted) and opened in the RunSnakeRun tool.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">To make sure the line numbers in the profiling data correctly
match the source files being viewed in RunSnakeRun, the correct
revision of the project is checked out before opening it in the
external GUI tool.</p>
</div>
<p>You can also get the raw profiling data by using the <tt class="docutils literal"><span class="pre">--output</span></tt>
argument to <tt class="docutils literal"><span class="pre">asv</span> <span class="pre">profile</span></tt>.</p>
</div>
<div class="section" id="comparing-the-benchmarking-results-for-two-revisions">
<span id="comparing"></span><h2>Comparing the benchmarking results for two revisions<a class="headerlink" href="#comparing-the-benchmarking-results-for-two-revisions" title="Permalink to this headline">¶</a></h2>
<p>In some cases, you may want to directly compare the results for two specific
revisions of the project. You can do so with the <tt class="docutils literal"><span class="pre">compare</span></tt> command:</p>
<div class="highlight-python"><div class="highlight"><pre>$ asv compare 7810d6d7 19aa5743
· Fetching recent changes.

All benchmarks:

    before     after       ratio
  [7810d6d7] [19aa5743]
+    1.75ms   152.84ms     87.28  time_quantity.time_quantity_array_conversion
+  933.71μs   108.22ms    115.90  time_quantity.time_quantity_init_array
    83.65μs    55.38μs      0.66  time_quantity.time_quantity_init_scalar
   281.71μs   146.88μs      0.52  time_quantity.time_quantity_scalar_conversion
+    1.31ms     7.75ms      5.91  time_quantity.time_quantity_ufunc_sin
      5.73m      5.73m      1.00  time_units.mem_unit
...
</pre></div>
</div>
<p>This will show the times for each benchmark for the first and second
revision, and the ratio of the second to the first. In addition, the
benchmarks will be color coded green and red if the benchmark improves
or worsens more than a certain threshold factor, which defaults to 2
(that is, benchmarks that improve by more than a factor of 2 or worsen
by a factor of 2 are color coded). The threshold can be set with the
<tt class="docutils literal"><span class="pre">--threshold=value</span></tt> option. Finally, the benchmarks can be split
into ones that have improved, stayed the same, and worsened, using the
same threshold.</p>
</div>
</div>


    </div>
      
  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-right">
      <a href="#">Back to top</a>
      
    </p>
    <p>
        &copy; Copyright 2013, Michael Droettboom.<br/>
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.2.3.<br/>
    </p>
  </div>
</footer>
  </body>
</html>